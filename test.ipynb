{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 10:55:16.518478: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-25 10:55:16.518889: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 768)    7680000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 768)   2362368     ['embedding[0][0]',              \n",
      " dAttention)                                                      'embedding[0][0]',              \n",
      "                                                                  'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, None, 768)   1536        ['multi_head_attention[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, None, 768)    787712      ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 768)    0           ['layer_normalization[0][0]',    \n",
      "                                                                  'sequential[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, None, 768)   1536        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 1)      769         ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,833,921\n",
      "Trainable params: 10,833,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define some hyperparameters\n",
    "vocab_size = 10000  # Size of the vocabulary\n",
    "embedding_dim = 768  # Dimensionality of the embeddings\n",
    "max_length = 50  # Maximum length of the input sequence\n",
    "num_heads = 8  # Number of attention heads in the multi-head attention mechanism\n",
    "ffn_units = 512  # Number of units in the feed-forward neural network\n",
    "\n",
    "# Define the input layer\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Define the transformer layer\n",
    "# 1. Multi-Head Self Attention Mechanism\n",
    "multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim // num_heads)\n",
    "x = multi_head_attention(query=x, value=x, key=x)\n",
    "x = layers.LayerNormalization()(x)\n",
    "\n",
    "# 2. Position-wise Feed-Forward Neural Network\n",
    "ffn = tf.keras.Sequential([\n",
    "    layers.Dense(ffn_units, activation='relu'),\n",
    "    layers.Dense(embedding_dim)\n",
    "])\n",
    "ffn_output = ffn(x)\n",
    "\n",
    "x = layers.Add()([x, ffn_output])\n",
    "x = layers.LayerNormalization()(x)\n",
    "\n",
    "# Define the output layer (for a binary classification task as an example)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm64_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
